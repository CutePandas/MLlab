{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "import jupyter\n",
    "import math\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot\n",
    "\n",
    "x, y_train = load_svmlight_file(\"a9atrain.txt\")\n",
    "x_train = x.toarray()\n",
    "x, y_test = load_svmlight_file(\"a9atest.txt\")\n",
    "x_test = x.toarray()\n",
    "X_train = numpy.hstack([x_train, numpy.ones((x_train.shape[0], 1))])\n",
    "X_test = numpy.hstack([x_test, numpy.zeros((x_test.shape[0], 1))])\n",
    "X_test = numpy.hstack([X_test, numpy.ones((x_test.shape[0], 1))])\n",
    "\n",
    "def compute_grad(x, y, w):\n",
    "    gradient = x * (y - x.dot(w.T))\n",
    "    return gradient\n",
    "\n",
    "def compute_loss(x, y, w, random_i):\n",
    "    loss = 0\n",
    "    a = len(random_i)\n",
    "    for m in range(a):\n",
    "        loss += 0.5 * ((y[random_i[m]] - x[random_i[m],:].dot(w.T)) ** 2)\n",
    "    return loss/a\n",
    "\n",
    "def NAG_train(x, y, x_test, y_test, w, C, lr, gamma, threshold, iteration):\n",
    "    vt = numpy.zeros(w.shape)\n",
    "    loss_history = []\n",
    "    test_loss_history = []\n",
    "    random_index = []\n",
    "    random_test_index = []\n",
    "    for i in range(iteration):\n",
    "        random_num = random.randint(0, x.shape[0]-1)\n",
    "        random_test_num = random.randint(0, x_test.shape[0]-1)\n",
    "        random_index.append(random_num)\n",
    "        random_test_index.append(random_test_num)\n",
    "        \n",
    "        \n",
    "    for i in range(iteration):\n",
    "        gradient = compute_grad(x[random_index[i],:], y[random_index[i]], w-gamma*vt)\n",
    "        vt = gamma*vt - lr*gradient\n",
    "        w -= vt\n",
    "        loss = compute_loss(x, y, w, random_index)\n",
    "        loss_history.append(loss)\n",
    "        test_loss_history.append(compute_loss(x_test, y_test, w, random_test_index))\n",
    "        if loss < threshold :\n",
    "             break \n",
    "    return w, loss_history, test_loss_history\n",
    "\n",
    "def RMSProp_train(x, y, x_test, y_test, w, C, lr, gamma, threshold, iteration):\n",
    "    Gt = 0\n",
    "    loss_history = []\n",
    "    test_loss_history = []\n",
    "    random_index = []\n",
    "    random_test_index = []\n",
    "    for i in range(iteration):\n",
    "        random_num = random.randint(0, x.shape[0]-1)\n",
    "        random_test_num = random.randint(0, x_test.shape[0]-1)\n",
    "        random_index.append(random_num)\n",
    "        random_test_index.append(random_test_num)\n",
    "        \n",
    "        \n",
    "    for i in range(iteration):\n",
    "        gradient = compute_grad(x[random_index[i],:], y[random_index[i]], w)\n",
    "        Gt = gamma*Gt + (1-gamma)*gradient.dot(gradient.T)\n",
    "        w += lr * gradient / math.sqrt(Gt+1e-8)\n",
    "        loss = compute_loss(x, y, w, random_index)\n",
    "        loss_history.append(loss)\n",
    "        test_loss_history.append(compute_loss(x_test, y_test, w, random_test_index))\n",
    "        if loss < threshold : \n",
    "              break \n",
    "    return w, loss_history, test_loss_history\n",
    "\n",
    "def AdaDelta_train(x, y, x_test, y_test, w, C, lr, gamma, threshold, iteration):\n",
    "    Gt = 0\n",
    "    variable_t = 0\n",
    "    loss_history = []\n",
    "    test_loss_history = []\n",
    "    random_index = []\n",
    "    random_test_index = []\n",
    "    for i in range(iteration):\n",
    "        random_num = random.randint(0, x.shape[0]-1)\n",
    "        random_test_num = random.randint(0, x_test.shape[0]-1)\n",
    "        random_index.append(random_num)\n",
    "        random_test_index.append(random_test_num)\n",
    "        \n",
    "        \n",
    "    for i in range(iteration):\n",
    "        gradient = compute_grad(x[random_index[i],:], y[random_index[i]], w) \n",
    "        Gt = gamma*Gt + (1-gamma)*gradient.dot(gradient.T)\n",
    "        variable_w = - math.sqrt(variable_t + 1e-8) * gradient / math.sqrt(Gt + 1e-8)\n",
    "        w -= variable_w\n",
    "        variable_t = gamma*variable_t + (1-gamma)*variable_w.dot(variable_w.T)\n",
    "        loss = compute_loss(x, y, w, random_index)\n",
    "        loss_history.append(loss)\n",
    "        test_loss_history.append(compute_loss(x_test, y_test, w, random_test_index))\n",
    "        if loss < threshold :\n",
    "              break \n",
    "    return w, loss_history, test_loss_history\n",
    "\n",
    "def Adam_train(x, y, x_test, y_test, w, C, lr, gamma, threshold, iteration):\n",
    "    Gt = 0\n",
    "    moment = numpy.zeros((1, x.shape[1]))\n",
    "    B = 0.9\n",
    "    loss_history = []\n",
    "    test_loss_history = []\n",
    "    random_index = []\n",
    "    random_test_index = []\n",
    "    for i in range(iteration):\n",
    "        random_num = random.randint(0, x.shape[0]-1)\n",
    "        random_test_num = random.randint(0, x_test.shape[0]-1)\n",
    "        random_index.append(random_num)\n",
    "        random_test_index.append(random_test_num)\n",
    "        \n",
    "        \n",
    "    for i in range(iteration):\n",
    "        gradient = compute_grad(x[random_index[i],:], y[random_index[i]], w)\n",
    "        moment = B*moment + (1-B)*gradient\n",
    "        Gt = gamma*Gt + (1-gamma)*gradient.dot(gradient.T)\n",
    "        a = lr * math.sqrt(1 - pow(gamma, iteration)) / (1-pow(B, iteration))\n",
    "        w += a * moment / math.sqrt(Gt + 1e-8)\n",
    "        loss = compute_loss(x, y, w, random_index)\n",
    "        loss_history.append(loss)\n",
    "        test_loss_history.append(compute_loss(x_test, y_test, w, random_test_index))\n",
    "    if loss < threshold :\n",
    "            break \n",
    "    return w, loss_history, test_loss_history\n",
    "\n",
    "iteration = 3000\n",
    "NAG_w = numpy.zeros((1, X_train.shape[1]))\n",
    "NAG_w, NAG_loss_history, NAG_test_loss_history = NAG_train(X_train, y_train, X_test, y_test, NAG_w, 0.3, 0.001, 0.9, 0.001, iteration)\n",
    "\n",
    "RMS_w = numpy.zeros((1, X_train.shape[1]))\n",
    "RMS_w, RMS_loss_history, RMS_test_loss_history = RMSProp_train(X_train, y_train, X_test, y_test, RMS_w, 0.3, 0.001, 0.9, 0.001, iteration)\n",
    "\n",
    "AdaDelta_w = numpy.zeros((1, X_train.shape[1]))\n",
    "AdaDelta_w, AdaDelta_loss_history, AdaDelta_test_loss_history = AdaDelta_train(X_train, y_train, X_test, y_test, AdaDelta_w, 0.3, 0.001, 0.9, 0.001, iteration)\n",
    "\n",
    "Adam_w = numpy.zeros((1, X_train.shape[1]))\n",
    "Adam_w, Adam_loss_history, Adam_test_loss_history = Adam_train(X_train, y_train, X_test, y_test, Adam_w, 0.3, 0.001, 0.9, 0.001, iteration)\n",
    "\n",
    "\n",
    "pyplot.plot(NAG_test_loss_history, label = 'NAG_validation_loss')\n",
    "pyplot.plot(RMS_test_loss_history, label = 'RMSProp_validation_loss')\n",
    "pyplot.plot(AdaDelta_test_loss_history, label = 'AdaDelta_validation_loss')\n",
    "pyplot.plot(Adam_test_loss_history, label = 'Adam_validation_loss')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.ylabel('loss')\n",
    "pyplot.xlabel('iteration')\n",
    "pyplot.title('graph')\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
