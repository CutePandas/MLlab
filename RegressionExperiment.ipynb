{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, model_selection, linear_model\n",
    "import numpy as np\n",
    "import jupyter\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "X_train, y_train = datasets.load_svmlight_file(\"a9atrain.txt\")\n",
    "\n",
    "x_ = np.array(X_train.toarray(), np.float32).reshape((-1, 123))\n",
    "y_ = np.array(y_train, np.float32).reshape((-1, 1))\n",
    "for i in range(y_.shape[0]):\n",
    "    if y_[i,0] == -1.0 : y_[i,0] = 0.\n",
    "\n",
    "X_ = np.hstack([x_, np.ones((x_.shape[0], 1))])\n",
    "X_test, y_test = datasets.load_svmlight_file(\"a9atest.txt\")\n",
    "xt_ = np.array(X_test.toarray(), np.float32).reshape((-1, 122))\n",
    "yt_ = np.array(y_test, np.float32).reshape((-1, 1))\n",
    "for i in range(yt_.shape[0]):\n",
    "    if yt_[i,0] == -1.0 : yt_[i,0] = 0.\n",
    "\n",
    "\n",
    "Xt_ = np.hstack([xt_, np.zeros((xt_.shape[0], 1)),np.ones((xt_.shape[0], 1))])\n",
    "\n",
    "def h_theta(Xi, Theta):\n",
    "    e_t = math.exp(Xi.dot(Theta.T))\n",
    "    return e_t / (1 + e_t)\n",
    "def compute_loss(X, y, Theta):\n",
    "    m = y.shape[0]\n",
    "    loss = 0.\n",
    "    for i in range(m):\n",
    "         loss +=  (y[i] * math.log(h_theta(X[i, :], Theta))) + ((1 - y[i]) * math.log(1 - h_theta(X[i,:], Theta)))\n",
    "    loss /= - m\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_gradient(X, y, Theta):\n",
    "    m = y.shape[0]\n",
    "    gradient = np.zeros(Theta.shape)\n",
    "    for i in range(m):\n",
    "        gradient += (h_theta(X[i, :], Theta) - y[i]) * (X[i, :])\n",
    "    gradient /= m\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def train_model_nag(X, y, Theta, learning_rate, gamma, iteration = 10000):\n",
    "    test_loss_history = np.zeros((iteration, 1))\n",
    "    v = np.zeros(Theta.shape)\n",
    "    Theta_gradient = np.zeros(Theta.shape)\n",
    "    \n",
    "    for iter in range(iteration):\n",
    "        index = random.randint(0, y.shape[0]-10)\n",
    "        Theta = Theta - gamma * v\n",
    "        v = gamma * v - learning_rate * compute_gradient(X[index:index+10,:], y[index:index+10], Theta)\n",
    "        Theta = Theta + v\n",
    "        test_loss_history[iter] = compute_loss(Xt_, yt_, Theta)[-1:]\n",
    "    return test_loss_history, Theta\n",
    "\n",
    "\n",
    "def train_model_rmsprop(X, y, Theta, learning_rate, gamma, epsilon, iteration = 10000):\n",
    "    test_loss_history = np.zeros((iteration, 1))\n",
    "    G_t = 0.\n",
    "    Theta_gradient = np.zeros(Theta.shape)\n",
    "    \n",
    "    for iter in range(iteration):\n",
    "        index = random.randint(0, y.shape[0]-10)\n",
    "        Theta_gradient = compute_gradient(X[index:index+10,:], y[index:index+10], Theta)\n",
    "        G_t = gamma * G_t + (1 - gamma) * Theta_gradient.dot(Theta_gradient.T)\n",
    "        Theta = Theta - (learning_rate / np.sqrt(G_t + epsilon)) * Theta_gradient\n",
    "        test_loss_history[iter] = compute_loss(Xt_, yt_, Theta)[-1:]\n",
    "    return test_loss_history, Theta\n",
    "\n",
    "\n",
    "def train_model_adadelta(X, y, Theta, gamma, epsilon, iteration):\n",
    "    test_loss_history = np.zeros((iteration, 1))\n",
    "    Theta_gradient = np.zeros(Theta.shape)\n",
    "    G_t = 0.\n",
    "    delta_theta = np.zeros(Theta.shape)\n",
    "    delta_t = 0.03\n",
    "    for iter in range(iteration):\n",
    "        index = random.randint(0, y.shape[0]-10)\n",
    "        Theta_gradient = compute_gradient(X[index:index+10,:], y[index:index+10], Theta)\n",
    "        G_t = gamma * G_t + (1 - gamma) * Theta_gradient.dot(Theta_gradient.T)\n",
    "        delta_theta = - (np.sqrt(delta_t + epsilon) / np.sqrt(G_t + epsilon)) * Theta_gradient\n",
    "        Theta = Theta + delta_theta\n",
    "        delta_t = gamma * delta_t + (1 - gamma) * (delta_theta.dot(delta_theta.T))\n",
    "        test_loss_history[iter] = compute_loss(Xt_, yt_, Theta)[-1:]\n",
    "    return test_loss_history, Theta\n",
    "\n",
    "\n",
    "\n",
    "def train_model_adam(X, y, Theta, learning_rate, beta1, beta2, epsilon, iteration):\n",
    "    test_loss_history = np.zeros((iteration, 1))\n",
    "    Theta_gradient = np.zeros(Theta.shape)\n",
    "    v_t = 0.\n",
    "    m_t = np.zeros(Theta.shape)\n",
    "    for iter in range(iteration):\n",
    "        index = random.randint(0, y.shape[0]-10)\n",
    "        Theta_gradient = compute_gradient(X[index:index+10,:], y[index:index+10], Theta)\n",
    "        m_t = beta1 * m_t + (1 - beta1) * Theta_gradient\n",
    "        v_t = beta2 * v_t + (1 - beta2) * Theta_gradient.dot(Theta_gradient.T)\n",
    "        mt_estimate = m_t / (1 - pow(beta1, iter + 1))\n",
    "        vt_estimate = v_t / (1 - pow(beta2, iter + 1))\n",
    "        Theta = Theta - learning_rate * mt_estimate / (np.sqrt(vt_estimate) + epsilon)\n",
    "        test_loss_history[iter] = compute_loss(Xt_, yt_, Theta)[-1:]\n",
    "    return test_loss_history, Theta\n",
    "\n",
    "iteration = 1000\n",
    "be1 = 0.9\n",
    "be2 = 0.999\n",
    "ep = 1e-8\n",
    "t_nag = np.zeros((1, 124))\n",
    "t_rmsprop = np.zeros((1, 124))\n",
    "t_adadelta = np.zeros((1, 124))\n",
    "t_adam = np.zeros((1, 124))\n",
    "\n",
    "\n",
    "\n",
    "nag_loss_history, t_nag = train_model_nag(X_, y_, t_nag, 0.005, be1, iteration)\n",
    "rmsprop_loss_history, t_rmsprop = train_model_rmsprop(X_, y_, t_rmsprop, 0.005, be1, ep, iteration)\n",
    "adadelta_loss_history, t_adadelta = train_model_adadelta(X_, y_, t_adadelta, be1, ep, iteration)\n",
    "adam_loss_history, t_adam = train_model_adam(X_, y_, t_adam, 0.005, be1, be2, ep, iteration)\n",
    "\n",
    "\n",
    "plt.plot(nag_loss_history, 'g', label='NAG')\n",
    "plt.plot(rmsprop_loss_history, 'b', label='RMSProp')\n",
    "plt.plot(adadelta_loss_history, 'r', label='AdaDelta')\n",
    "plt.plot(adam_loss_history, 'y', label='Adam')\n",
    "\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.ylabel('lost');\n",
    "\n",
    "plt.xlabel('iteration count')\n",
    "\n",
    "plt.title('loss graph')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
